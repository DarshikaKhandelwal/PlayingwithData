# -*- coding: utf-8 -*-
"""DataPlaying.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FUzEtQFQhmyaS_bpi8ozCr2yAjXtkACl
"""

!pip install bs4

!pip install requests

!pip install bs4

!pip install nltk

from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize, LineTokenizer, SpaceTokenizer, BlanklineTokenizer

import nltk
nltk.download('punkt', quiet = 'True')

import pandas as pd
df = pd.read_csv('/content/Input.xlsx - Sheet1.csv')
url_ids = []
urls = []
for rows in range(114):
  urls.append(df.loc[rows,'URL'])

urls

file_name = ""
file_names = []
count = 0
for i in range(37,151):
  file_name = "file_"+str(i)+".txt"
  file_names.append(file_name)
  count += 1
file = open(file_name,'x')
print('created',count,'files')

file_names[23]

import requests
from bs4 import BeautifulSoup
for i in range(114):
  response = requests.get(urls[i])
  soup = BeautifulSoup(response.text, 'html.parser')
  lines = soup.find('body').find_all('p')
  file = open(file_names[i],'w')
  for x in lines:
    file.write(x.text.strip())
  file.close()

import requests
from bs4 import BeautifulSoup
response = requests.get('https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/')
soup = BeautifulSoup(response.text, 'html.parser')
lines = soup.find('body').find_all('p')
file = open('text_file.txt','w')
for x in lines:
  file.write(x.text.strip())

# import os
# for i in range(114):
#   if os.path.exists(file_names[i]):
#     os.remove(file_names[i])
#   else:
#     continue

from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob

# the sentiments of the text are shown by the sentiment analyzer
def positive_sentiment_analyze(sentiment_text):
  score = SentimentIntensityAnalyzer().polarity_scores(sentiment_text)
  return score['pos']
def negative_sentiment_analyze(sentiment_text):
  score = SentimentIntensityAnalyzer().polarity_scores(sentiment_text)
  return score['neg']
# for polarity scores, we use blob by the textblob module in python
def polarity_analyze(sentiment_text):
  blob = TextBlob(sentiment_text)
  return ('%.2f' % blob.polarity)
# for subjectivity scores, we use blob by the textblob module in python
def polarity_analyze(sentiment_text):
  blob = TextBlob(sentiment_text)
  return blob.subjectivity

nltk.downloader.download('vader_lexicon')

output_df = pd.read_csv('/content/Output Data Structure.xlsx - Sheet1.csv')
output_df

positive_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  positive_list.append(positive_sentiment_analyze(data))

negative_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  negative_list.append(negative_sentiment_analyze(data))

polarity_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  polarity_list.append(polarity_analyze(data))
polarity_list

# average sentence lenght
def avg_sentence_len(text):
  sentences = text.split(".")
  words = text.split(" ")
  avg_sentence_len = len(words) / len(sentences)
  return ('%.2f' % avg_sentence_len)

average_len_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  average_len_list.append(avg_sentence_len(data))

# getting complex words
def percentage_complex_words(data):
  vowels = ['a','e','i','o','u']
  words_list = []
  count = 0
  complex_words = 0
  words_list.append(data.split(" "))
  for i in words_list:
    letters = []
    for j in range(len(i)):
      letters.append(i[j])
      if letters[j] in vowels:
        count += 1
      if count >= 2:
        complex_words += 1
  try:
    percentage = (complex_words / len(data)) * 100
  except ZeroDivisionError:
    percentage = 0
  return('%.2f' % percentage)

# getting complex words_count
def complex_words(data):
  vowels = ['a','e','i','o','u']
  words_list = []
  count = 0
  complex_words = 0
  words_list = data.split(" ")
  for i in words_list:
    letters = []
    for j in range(len(i)):
      letters.append(i[j])
      if letters[j] in vowels:
        count += 1
      if count >= 2:
        complex_words += 1
  return('%.2f' % complex_words)

# personal pronouns
def personal_pronouns(data):
  personal_pronouns = ['I', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them']
  words_list = []
  count = 0
  words_list = data.split(' ')
  for i in words_list:
    if i in personal_pronouns:
      count += 1
  return count

# average word lenght
def avg_word_length(data):
  sum = 0
  word_length = []
  total_words = len(data)
  words_list = data.split(' ')
  for i in words_list:
    word_length.append(len(i))
  for i in word_length:
    sum += i
  len(word_length)
  avg = sum / len(word_length)
  return ('%.2f' % avg)

!pip install textstat

# getting the syllables per word
import textstat
def syllable_count(data):
  word_list = data.split(' ')
  count = 0
  syllable_list = []
  for i in word_list:
    syllable_list.append(textstat.syllable_count(i))
  for i in syllable_list:
    count += i
  average_syllables_per_word = count / len(word_list)
  return average_syllables_per_word

# getting a list of percentages of complex words
complex_word_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  complex_word_list.append(percentage_complex_words(data))

# getting a list of complex word count
complex_word_count_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  complex_word_count_list.append(complex_words(data))

# getting a list of word count
word_count_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  word_count_list.append(len(data))

# fog index using textstat
def fog_index(data):
  return textstat.gunning_fog(data)
###########
# we can also find the fog index without using the library 
# the formula is 
# 0.4[(words/sentences) + 100(complex_words/words)]

# getting a list of average word lenght in the text
avg_word_len_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  avg_word_len_list.append(avg_word_length(data))

# getting a list of personal pronouns
personal_pronoun_list = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  personal_pronoun_list.append(personal_pronouns(data))

# getting a list of syllables
syllables = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  syllables.append(syllable_count(data))

# getting fog index
fog_indices = []
for i in range(114):
  text = open(file_names[i],'r')
  data = text.read()
  fog_indices.append(fog_index(data))

# for positive scores of the text
for i in range(114):
  output_df.at[i,'POSITIVE SCORE'] = positive_list[i]
# for negative scores of the text
for i in range(114):
  output_df.at[i,'NEGATIVE SCORE'] = negative_list[i]
# for polarity score of the text
for i in range(114):
  output_df.at[i,'POLARITY SCORE'] = polarity_list[i]
# for subjectivity score of the text
for i in range(114):
  output_df.at[i,'SUBJECTIVITY SCORE'] = polarity_list[i]
# for average sentence lenght of the text
for i in range(114):
  output_df.at[i,'AVG SENTENCE LENGTH'] = average_len_list[i]
# for average number of words per sentence of the text
for i in range(114):
  output_df.at[i,'AVG NUMBER OF WORDS PER SENTENCE'] = average_len_list[i]
# for complex words of the text
for i in range(114):
  output_df.at[i,'PERCENTAGE OF COMPLEX WORDS'] = complex_word_list[i]
# for complex word_count of the text
for i in range(114):
  output_df.at[i,'COMPLEX WORD COUNT'] = complex_word_count_list[i]
# for word_count of the text
for i in range(114):
  output_df.at[i,'WORD COUNT'] = word_count_list[i]
# for personal_pronoun_count of the text
for i in range(114):
  output_df.at[i,'PERSONAL PRONOUNS'] = personal_pronoun_list[i]
# for average_word_length of the text
for i in range(114):
  output_df.at[i,'AVERAGE WORD LENGTH'] = avg_word_len_list[i]
for i in range(114):
  output_df.at[i,'AVG WORD LENGTH'] = avg_word_len_list[i]
# for average_syllables_per_word of the text
for i in range(114):
  output_df.at[i,'SYLLABLE PER WORD'] = syllables[i]
# for fog index of the text
for i in range(114):
  output_df.at[i,'FOG INDEX'] = fog_indices[i]

output_df.head()

output_df.to_excel('output_final.xlsx')